#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys, os, codecs, struct, cPickle, gc
import numpy as np
from scipy import sparse
from gensim.utils import lemmatize
from gensim.models import LdaModel
from gensim.corpora import Dictionary
from time import time


class EvalWordNeighbours:
    def __init__(self, embedding, vocabulary):
        self.w2idx_embedding = {}
        self.idx2w_embedding = {}
        self.embedding_norm = self._generate(vocabulary, embedding)
        self.cooccur_matrix = self._generate("", "assets/upto201709.text.lema.100count.from-Word2vec-vocab.cooccur.pkl")
        self.lda_dictionary, self.lda = self._load_lda_model('assets/lda_model/upto201709.lema.no_below_70docs.dict',
                                                             'assets/lda_model/upto201709.no_below_70docs.100k.ldamodel')

    def _open_vocabulary(self, filepath):
        vocab = codecs.open(filepath,'r','utf-8')
        vocab = list(vocab)

        # En dos pasos porque hay palabras sin TAG que no tienen /X
        words = [x.rstrip().split()[0] for x in vocab]
        words = [x.rstrip().split('/')[0] for x in words]

        w2idx = {w: idx for idx, w in enumerate(words)}
        idx2w = {idx: w for idx, w in enumerate(words)}

        self.w2idx_embedding = w2idx
        self.idx2w_embedding = idx2w

    def _unpack_cooccurrence_matrix(self, matrixpath):
        with open(matrixpath,'rb') as f:
            content = f.read()
        f.close()
        data_type = 'I I d'
        unpack_content = struct.unpack(data_type * ((len(content)//4)/4), content)

        total_cooccur = 0
        cooccur = sparse.lil_matrix((len(self.w2idx_embedding), len(self.w2idx_embedding)),dtype=np.float64)
        for i in range(0, len(unpack_content), 3):
            values = unpack_content[i:i+3]
            cooccur[values[0]-1, values[1]-1] = values[2]
            total_cooccur += values[2]
        return cooccur

    def _generate(self, vocabpath, datapath):
        if os.path.exists(vocabpath):
            self._open_vocabulary(vocabpath)

        # Open COMPLETE cooccurrence matrix generated by GloVe and unpack values
        if datapath.strip().split('.')[-2] == 'cooccur' and datapath.strip().split('.')[-1] == 'bin':
            vec = self._unpack_cooccurrence_matrix(datapath)
            gc.collect()
            return vec
        elif datapath.strip().split('.')[-1] == 'pkl':
            # vec = cPickle.load(open(datapath, 'rb'))
            vectors = sparse.lil_matrix((len(self.w2idx_embedding), len(self.w2idx_embedding)), dtype=np.float64)
            # for i,line in enumerate(vec):
            #     vectors[self.w2idx_embedding[self.idx2w_embedding[i]],:] = np.squeeze(line.toarray())
            # del vec
            # gc.collect()
            return vectors
        elif datapath.strip().split('.')[-1] == 'txt':
            with open(datapath, 'r') as f:
                vectors = {}
                for line in f:
                    vals = line.rstrip().split()
                    word = vals[0].decode('utf-8').split()[0]
                    word = word.split('/')[0]
                    vectors[word] = [float(x) for x in vals[1:]]
            f.close()
            gc.collect()
        elif datapath.strip().split('.')[-1] == 'data':
            with open(datapath, 'r') as f:
                vectors = {}
                for i,line in enumerate(f):
                    vals = line.rstrip().split()
                    vectors[self.idx2w_embedding[i]] = [float(x) for x in vals]
            f.close()
            gc.collect()
        else:
            print("Check vector matrix type")
            sys.exit()

        vocab_size = len(self.w2idx_embedding)
        vector_dim = len(vectors[self.idx2w_embedding[0]])
        W = np.zeros((vocab_size, vector_dim))
        for word, v in vectors.items():
            if word == u'<unk>' or word == u'<s>' or word == u'</s>':
                continue
            W[self.w2idx_embedding[word], :] = v

        # normalize each word vector to unit variance
        # d = (np.sum(W ** 2, 1) ** (0.5))
        # W_norm = (W.T / d).T
        W_norm = W / np.array([np.linalg.norm(W, axis=1)]).T

        del vectors
        gc.collect()
        return W_norm

    def _load_lda_model(self, dictionary_path, ldamodel_path):
        dictionary = Dictionary.load(dictionary_path)
        lda = LdaModel.load(ldamodel_path)
        return dictionary, lda

    def _distance_neighbour(self, input_term, N=20):
        if input_term in self.w2idx_embedding:
            vec_result = self.embedding_norm[self.w2idx_embedding[input_term], :]
        else:
            # palabra fuera de vocabulario
            return

        # vec_norm = np.zeros(vec_result.shape)
        # d = (np.sum(vec_result ** 2,) ** (0.5))
        # vec_norm = (vec_result.T / d).T

        # Ya esta normalizado, al igual que W
        vec_norm = vec_result
        dist = np.dot(self.embedding_norm, vec_norm.T)

        index = self.w2idx_embedding[input_term]
        dist[index] = -np.Inf

        a = np.argsort(-dist)

        gc.collect()
        return a[:N]

    def _distance_vector(self, vector1, vector2):
        if isinstance(vector1, list):
            vector1 = np.reshape(vector1, (1,len(vector1)))
        assert vector1.shape[0] == 1
        assert vector1.shape[1] == vector2.shape[1]

        dist = np.dot(vector2, vector1.T)

        id_sort_distance = np.argsort(-dist, axis=0)
        gc.collect()
        return int(id_sort_distance[0])

    def eval(self, word, picto_dict):
        neighb = self._distance_neighbour(word, N=30)
        if neighb is not None:
            for id_word in neighb:
                if self.idx2w_embedding[id_word] in picto_dict:
                    return self.idx2w_embedding[id_word]
                else:
                    pass
        else:
            return

   
    def eval_with_context_lda(self, sentence, wid, all_tags, stopwords):
        '''
        Se calcula embedding de frase mediante una suma ponderada de los vectores de cada palabra. Los pesos son el
        valor dado a cada palabra en el topic del documento por LDA.

        Se calcula distancia coseno entre el vector de frase y cada uno de los vectores de tags.

        :param sentence: frase de entrada (referencia)
        :param wid: posición de la palabra objetivo en la frase
        :param all_tags: lista de listas. Cada lista contiene todos los tags asociados a un pictograma
        :param stopwords: si figura alguna stopword entre los tags, no se tiene en cuenta
        :return: número del pictograma más representativo
        '''

        def _get_weights_lda(text, stopwords, lda_dictionary, lda, w2idx_embedding, target_word, lda_topics=False):
            my_punctuation = '!"#$%&\'()*+,-./:;<=>?@[]^_`{|}~'

            new_corpus = lda_dictionary.doc2bow(text)
            if not lda_topics:
                '''
                code removed
                '''
                
                
            number_of_topics_in_current_document = len(lda_topics)
            topic_word_matrix = lda.expElogbeta
            aux = np.reshape(lda_topics, (len(lda_topics), 2))
            topics = aux[:, 0]
            p_pertenencia_a_topics = aux[:,1]

            word2id_in_file = Dictionary([text]).token2id
            word2id_global_lda = lda_dictionary.token2id
            # Initialize a matrix whose values will be each word's weight.
            # Shape: [number_of_topics_in_current_document, number_of_words]
            topic_filewords_matrix = np.zeros((number_of_topics_in_current_document, len(word2id_in_file)), dtype=float)

            for k_top, topic in enumerate(topics):
                for word in word2id_in_file.keys():
                    '''
                    code removed
                    '''

            # Suma por columnas para tener el peso acumulado de una palabra en todos los topics del documento
            weight = np.sum(topic_filewords_matrix, axis=0)
            # Normalizo
            # w = weight / np.array([np.linalg.norm(weight, axis=0)]).T
            d = (np.sum(weight ** 2,) ** (0.5))
            weight = (weight.T / d).T

            weight_words = {word:ww for word,ww in zip(word2id_in_file.keys(), weight) if (ww != 0 and word in self.w2idx_embedding)}
            
            return weight_words, lda_topics

        target_word = sentence[wid]
        weights, lda_topic = _get_weights_lda(sentence, stopwords, self.lda_dictionary, self.lda, self.w2idx_embedding, target_word)
        words_id = [self.w2idx_embedding[word] for word in weights.keys()]
        weights_id = weights.values()

        sentence_vec = np.dot(weights_id, self.embedding_norm[words_id])
        sentence_vec = np.reshape(sentence_vec, (1,len(sentence_vec)))  # [1, embedding_size]

        # Calculo embedding de cada conjunto de tags. Resultado final es un embedding por picto.
        all_tags_vec = np.zeros((len(all_tags), sentence_vec.shape[1]))
        for i, set_of_tags in enumerate(all_tags):
            del weights, words_id
            set_of_tags = lemmatize(' '.join(set_of_tags), stopwords=stopwords)  # lematizo tags y quito (si hay) stopwords
            set_of_tags = [word.split('/')[0].decode('utf-8') for word in set_of_tags]  # quito postag añadido y codifico en utf8
            set_of_tags = [w for w in set_of_tags if w in self.w2idx_embedding]  # todas las palabras en vocabulario

            weights, _ = _get_weights_lda(set_of_tags, stopwords, self.lda_dictionary, self.lda, self.w2idx_embedding,
                                          target_word, lda_topics=lda_topic)
            words_id = [self.w2idx_embedding[word] for word in weights.keys()]
            weights_id = weights.values()

            tag_vec = np.dot(weights_id, self.embedding_norm[words_id])
            all_tags_vec[i,:] = tag_vec

        # Calculo distancias entre la frase original y cada uno de los pictogramas/tags
        picto_id = self._distance_vector(sentence_vec, all_tags_vec)
        return picto_id


def test():
    i1 = time()
    postag_file = "assets/upto201709.text.lema.tagged.200.wordcount.txt"
    embedding_file = "assets/upto201709.text.lema.tagged.200.vectors.txt"
    word2vec = EvalWordNeighbours(embedding_file, postag_file)

    with codecs.open('assets/stopwords_es_ES_PICTOS.txt', 'r', 'utf-8') as ff:
        stopwords = [line.strip() for line in ff]

    sentence = "en verano hacer foto a el banco de pez".split()
    tags = [["parque", "sentar", "madera"], ["dinero", "sacar", "efectivo"], ["conjunto","peces","mar","marino"]]
    picto_id = word2vec.eval_with_context_lda(sentence, 3, tags, stopwords)
    print("\nSelected picto : %d" % picto_id)
    i2 = time()
    print("\nElapsed time : %.6f seconds\n" % (i2-i1))

if __name__ == '__main__':
    test()
